{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73eca674",
   "metadata": {},
   "source": [
    "# Here We test <DB, Schema Independent Evaluators> \n",
    "- Test only with query, pred_sql, gold_sql\n",
    "- Step1: Evaluate with only Negative data with existing dataset -> Calculate FP, TN\n",
    "- Step2: Evaluate with full positive and negative data by adding hand-made poistive set -> Calculate TP ,FN  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54db0568",
   "metadata": {},
   "source": [
    "### Make Negative Evaluation-pipeline test set from nl2sql_bug set\n",
    "- Import nl2sql testest and create only-negative evaluation-pipeline test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8269f6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from azureml.fsspec import AzureMachineLearningFileSystem\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3011db",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "data_storage_uri = os.getenv(\"AZURE_DATASTORAGE_URI\")\n",
    "fs = AzureMachineLearningFileSystem(data_storage_uri)\n",
    "fs.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b98973",
   "metadata": {},
   "outputs": [],
   "source": [
    "with fs.open('./UI/2025-07-25_003545_UTC/NL2SQL-Bugs-with-evidence.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "nl2sql_bug_df = pd.DataFrame(data)\n",
    "nl2sql_bug_df[\"label\"] = nl2sql_bug_df[\"label\"].astype(bool)\n",
    "len(nl2sql_bug_df.loc[nl2sql_bug_df[\"label\"]== True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df8cb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 같은 Question은 공유하고 SQL은 다르게 적힌 True 있나 확인\n",
    "dupes = (\n",
    "    nl2sql_bug_df[nl2sql_bug_df[\"label\"] == True]\n",
    "    .groupby(\"question\")\n",
    "    #그룹 객체 대상으로 람다실행하고, 조건을 마족하는 원래 DataFrame의 row반환\n",
    "    .filter(lambda x: len(x) > 1)\n",
    ")\n",
    "print(dupes) # 중복된 질문을 가진 True 라벨 row의 수\n",
    "print(dupes[\"question\"].unique())  # 어떤 question인지 보기\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3884dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0) Make sure your label column is really boolean\n",
    "#    (if it's the strings \"True\"/\"False\" or ints 1/0, convert it)\n",
    "nl2sql_bug_df[\"label\"] = nl2sql_bug_df[\"label\"].astype(bool)\n",
    "\n",
    "# 1) Build a map from question → gold_sql (there must be at most one per question)\n",
    "gold_map = (\n",
    "    nl2sql_bug_df\n",
    "    # 조건을 이용해 행선택하는 함수 \n",
    "    .loc[nl2sql_bug_df[\"label\"]]\n",
    "    # 특정열을 인덱스로만 바꾸고, sql 만 선택  \n",
    "    .set_index(\"question\")[\"sql\"]\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "len(gold_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e0752e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Walk through all FALSE‐label rows and emit one entry per pred_sql\n",
    "eval_pipeline_test_list = []\n",
    "for _, row in nl2sql_bug_df.loc[~nl2sql_bug_df[\"label\"]].iterrows():\n",
    "    q    = row[\"question\"]\n",
    "    pred = row[\"sql\"]\n",
    "    eval_pipeline_test_list.append({\n",
    "        \"question\":    q,\n",
    "        \"db_id\" : row[\"db_id\"],\n",
    "        \"gold_sql\":    gold_map.get(q),       # ← will be None only if no True‐row ever existed\n",
    "        \"pred_sql\":    pred,\n",
    "        \"label\":       False,\n",
    "        \"evidence\":    row[\"evidence\"],\n",
    "        \"error_types\": row[\"error_types\"],\n",
    "    })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2fa26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_pipeline_test_df = pd.DataFrame(eval_pipeline_test_list)\n",
    "eval_pipeline_test_df[\"gold_sql\"].isnull().sum()\n",
    "eval_pipeline_test_df = eval_pipeline_test_df.loc[eval_pipeline_test_df[\"gold_sql\"].notnull()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bf8204",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_pipeline_test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43e9581",
   "metadata": {},
   "source": [
    "### Evaluate llm-as-judge-raw-sql-evaluators \n",
    "- Testing with only False dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78726cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from evaluators.llm_as_judge_raw_sql_evaluator import LLMasJudgeRawSQL\n",
    "from azure.ai.evaluation import AzureOpenAIModelConfiguration\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "model_config_4o = AzureOpenAIModelConfiguration(\n",
    "    azure_endpoint= os.environ[\"AZURE_ENDPOINT\"],\n",
    "    azure_key = os.environ[\"AZURE_API_KEY\"],\n",
    "    azure_deployment = os.environ[\"AZURE_4O_DEPLOYMENT\"],\n",
    "    api_version = os.environ[\"AZURE_4O_API_VERSION\"]\n",
    ")\n",
    "\n",
    "model_config_o4_mini = AzureOpenAIModelConfiguration(\n",
    "    azure_endpoint= os.environ[\"AZURE_ENDPOINT\"],\n",
    "    azure_key = os.environ[\"AZURE_API_KEY\"],\n",
    "    azure_deployment = os.environ[\"AZURE_O4_MINI_DEPLOYMENT\"],\n",
    "    api_version = os.environ[\"AZURE_O4_MINI_API_VERSION\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529bcb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "llm_sql_evaluator = LLMasJudgeRawSQL(model_config=model_config_o4_mini)\n",
    "llm_raw_sql_pipeline_eval_result = []\n",
    "\n",
    "for i, row in tqdm(eval_pipeline_test_df.iterrows(), ncols=100, colour=\"cyan\", total=len(eval_pipeline_test_df)):\n",
    "    question = row[\"question\"]\n",
    "    gold_sql = row[\"gold_sql\"]\n",
    "    pred_sql = row[\"pred_sql\"]\n",
    "\n",
    "    try:\n",
    "        result = llm_sql_evaluator(question=question, gold_sql=gold_sql, pred_sql=pred_sql)\n",
    "        result = json.loads(result)\n",
    "        llm_raw_sql_pipeline_eval_result.append({\n",
    "            \"question\": question,\n",
    "            \"gold_sql\": gold_sql,\n",
    "            \"pred_sql\": pred_sql,\n",
    "            \"llm_judgement\": result[\"label\"],\n",
    "            \"reason\": result[\"reason\"]\n",
    "        })\n",
    "        print(f\"##### {i}th DEBUG LOG #####\")\n",
    "        print(\"Question:\", question)\n",
    "        print(\"Gold Sql:\", gold_sql)\n",
    "        print(\"Pred Sql:\", pred_sql)\n",
    "        print(\"LLM Judgement:\", result[\"label\"])\n",
    "        print(\"Reason:\", result[\"reason\"])\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"❌ JSONDecodeError at row {i}: {e}\")\n",
    "        print(\"⚠️ Raw response was:\")\n",
    "        print(repr(result))  # repr을 사용하면 문제되는 이스케이프 문자 확인 가능\n",
    "        llm_raw_sql_pipeline_eval_result.append({\n",
    "            \"question\": question,\n",
    "            \"gold_sql\": gold_sql,\n",
    "            \"pred_sql\": pred_sql,\n",
    "            \"llm_judgement\": \"ERROR\",\n",
    "            \"reason\": f\"JSONDecodeError: {str(e)}\"\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Unexpected error at row {i}: {e}\")\n",
    "        llm_raw_sql_pipeline_eval_result.append({\n",
    "            \"question\": question,\n",
    "            \"gold_sql\": gold_sql,\n",
    "            \"pred_sql\": pred_sql,\n",
    "            \"llm_judgement\": \"ERROR\",\n",
    "            \"reason\": f\"Exception: {str(e)}\"\n",
    "        })\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7d2962",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_raw_sql_pipeline_eval_df = pd.DataFrame(llm_raw_sql_pipeline_eval_result)\n",
    "\n",
    "#Join with Original Eval test set \n",
    "\n",
    "merged_llm_raw_sql_pipeline_eval_df = pd.merge(eval_pipeline_test_df, llm_raw_sql_pipeline_eval_df, on=\"pred_sql\", how='left')\n",
    "merged_llm_raw_sql_pipeline_eval_df.drop([\"question_y\",\"gold_sql_y\"], axis = 1)\n",
    "\n",
    "fp = len(merged_llm_raw_sql_pipeline_eval_df[merged_llm_raw_sql_pipeline_eval_df[\"llm_judgement\"] == \"correct\"])\n",
    "tn = len(merged_llm_raw_sql_pipeline_eval_df[merged_llm_raw_sql_pipeline_eval_df[\"llm_judgement\"] == \"incorrect\"])\n",
    "\n",
    "\n",
    "print(\"Total:\", len(merged_llm_raw_sql_pipeline_eval_df))\n",
    "print(\"FP:\", fp)\n",
    "print(\"TN:\", tn)\n",
    "print(fp/tn*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd1c919",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_llm_raw_sql_pipeline_eval_df[merged_llm_raw_sql_pipeline_eval_df[\"llm_judgement\"] == \"correct\"][\"error_types\"])\n",
    "\n",
    "total_errortypes_count = {}\n",
    "for _, row in merged_llm_raw_sql_pipeline_eval_df.iterrows():\n",
    "    error_type = row[\"error_types\"][0][\"error_type\"]\n",
    "    sub_error_type = row[\"error_types\"][0][\"sub_error_type\"]\n",
    "    key_tuple = (error_type,sub_error_type)\n",
    "    total_errortypes_count[key_tuple] = total_errortypes_count.get(key_tuple,0) +1\n",
    "\n",
    "print(total_errortypes_count)\n",
    "\n",
    "fp_errortypes_count = {}\n",
    "for _, row in merged_llm_raw_sql_pipeline_eval_df[merged_llm_raw_sql_pipeline_eval_df[\"llm_judgement\"] == \"correct\"].iterrows():\n",
    "    error_type = row[\"error_types\"][0][\"error_type\"]\n",
    "    sub_error_type = row[\"error_types\"][0][\"sub_error_type\"]\n",
    "    key_tuple = (error_type,sub_error_type)\n",
    "    fp_errortypes_count[key_tuple] = fp_errortypes_count.get(key_tuple,0) +1\n",
    "\n",
    "print(fp_errortypes_count)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22057b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 두 딕셔너리의 모든 고유 키를 추출\n",
    "all_keys = sorted(list(set(list(total_errortypes_count.keys()) + list(fp_errortypes_count.keys()))))\n",
    "\n",
    "# 모든 키를 포함하고 누락된 키는 0으로 채운 완전한 딕셔너리 생성\n",
    "full_dist1 = {key: total_errortypes_count.get(key, 0) for key in all_keys}\n",
    "full_dist2 = {key: fp_errortypes_count.get(key, 0) for key in all_keys}\n",
    "\n",
    "# DataFrame으로 변환 (시각화를 위해 긴 형식으로 변환)\n",
    "df1 = pd.DataFrame(list(full_dist1.items()), columns=['Error Type', 'Count'])\n",
    "df1['Distribution'] = 'Distribution 1'\n",
    "\n",
    "df2 = pd.DataFrame(list(full_dist2.items()), columns=['Error Type', 'Count'])\n",
    "df2['Distribution'] = 'Distribution 2'\n",
    "\n",
    "# 두 데이터프레임을 하나로 합치기\n",
    "combined_df = pd.concat([df1, df2])\n",
    "\n",
    "# 오류 카테고리와 유형 분리 (튜플 형태의 키를 분리)\n",
    "combined_df[['Error Category', 'Error Subtype']] = pd.DataFrame(combined_df['Error Type'].tolist(), index=combined_df.index)\n",
    "\n",
    "# 시각화를 위한 정렬 (Count가 높은 순서대로)\n",
    "# 모든 오류 유형에 대한 총 Count를 기준으로 정렬\n",
    "error_order = combined_df.groupby('Error Subtype')['Count'].sum().sort_values(ascending=False).index\n",
    "\n",
    "# 시각화 설정\n",
    "plt.figure(figsize=(16, 10)) # 그래프 크기 설정\n",
    "sns.set_theme(style=\"whitegrid\") # 스타일 설정\n",
    "\n",
    "# 막대 그래프 그리기 (오류 유형별로 Distribution 1과 Distribution 2 비교)\n",
    "ax = sns.barplot(\n",
    "    x='Count',\n",
    "    y='Error Subtype',\n",
    "    hue='Distribution',\n",
    "    data=combined_df,\n",
    "    order=error_order,\n",
    "    palette='viridis' # 색상 팔레트 설정\n",
    ")\n",
    "\n",
    "plt.title('Comparison of Error Distributions', fontsize=16) # 그래프 제목\n",
    "plt.xlabel('Number of Errors', fontsize=12) # x축 레이블\n",
    "plt.ylabel('Error Type', fontsize=12) # y축 레이블\n",
    "plt.legend(title='Distribution', fontsize=10, title_fontsize=12) # 범례\n",
    "\n",
    "# y축 레이블 텍스트가 잘리지 않도록 레이아웃 조정\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show() # 그래프 출력\n",
    "\n",
    "# ---\n",
    "# 보너스: 상대 빈도 비교 시각화 (더 명확한 비율 비교)\n",
    "# ---\n",
    "\n",
    "# 상대 빈도 계산 (이전 코드에서 계산했던 방식 재사용)\n",
    "total_dist1_count = df1['Count'].sum()\n",
    "total_dist2_count = df2['Count'].sum()\n",
    "\n",
    "df1['Relative Frequency (%)'] = (df1['Count'] / total_dist1_count) * 100\n",
    "df2['Relative Frequency (%)'] = (df2['Count'] / total_dist2_count) * 100\n",
    "\n",
    "combined_relative_df = pd.concat([df1, df2])\n",
    "\n",
    "# 상대 빈도를 기준으로 정렬 (Distribution 1의 상대 빈도를 기준으로)\n",
    "relative_error_order = combined_relative_df[combined_relative_df['Distribution'] == 'Distribution 1'] \\\n",
    "    .sort_values(by='Relative Frequency (%)', ascending=False)['Error Subtype']\n",
    "\n",
    "plt.figure(figsize=(16, 10))\n",
    "sns.barplot(\n",
    "    x='Relative Frequency (%)',\n",
    "    y='Error Subtype',\n",
    "    hue='Distribution',\n",
    "    data=combined_relative_df,\n",
    "    order=relative_error_order,\n",
    "    palette='magma' # 다른 색상 팔레트\n",
    ")\n",
    "\n",
    "plt.title('Comparison of Error Distributions by Relative Frequency', fontsize=16)\n",
    "plt.xlabel('Relative Frequency (%)', fontsize=12)\n",
    "plt.ylabel('Error Type', fontsize=12)\n",
    "plt.legend(title='Distribution', fontsize=10, title_fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb914372",
   "metadata": {},
   "source": [
    "### Evaluate Execution Match with LLM as a Judge\n",
    "- We will gonna implement Execution Match metrics and decide with LLM\n",
    "- We will qeury to database and retrieve result from it, using our pipeline-evaluation set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5eed6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd \n",
    "from db_utils.db_utils import get_db_path, execute_query, format_results_for_llm\n",
    "\n",
    "#Base_dir should be path for project root\n",
    "base_dir = os.path.abspath(\"..\")\n",
    "\n",
    "exec_match_test_df = eval_pipeline_test_df.copy()\n",
    "\n",
    "exec_match_test_df[\"gold_result\"] = None\n",
    "exec_match_test_df[\"pred_result\"] = None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deef5383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) 순회하며 실행 및 포매팅\n",
    "for idx, row in exec_match_test_df.iterrows():\n",
    "    db_id    = row[\"db_id\"]\n",
    "    gold_sql = row[\"gold_sql\"]\n",
    "    pred_sql = row[\"pred_sql\"]\n",
    "\n",
    "    try:\n",
    "        db_path = get_db_path(base_dir, db_id)\n",
    "    except FileNotFoundError as e:\n",
    "        exec_match_test_df.at[idx, \"gold_result\"] = f\"ERROR: {e}\"\n",
    "        exec_match_test_df.at[idx, \"pred_result\"] = f\"ERROR: {e}\"\n",
    "        continue\n",
    "\n",
    "    # gold 실행 → 포매팅\n",
    "    gold_raw = execute_query(db_path, gold_sql)\n",
    "    exec_match_test_df.at[idx, \"gold_result\"] = format_results_for_llm(\n",
    "        gold_raw,\n",
    "        sort_keys=list(gold_raw[0].keys()) if gold_raw else None,\n",
    "        row_limit=10\n",
    "    )\n",
    "\n",
    "    # pred 실행 → 포매팅\n",
    "    pred_raw = execute_query(db_path, pred_sql)\n",
    "    exec_match_test_df.at[idx, \"pred_result\"] = format_results_for_llm(\n",
    "        pred_raw,\n",
    "        sort_keys=list(pred_raw[0].keys()) if pred_raw else None,\n",
    "        row_limit=5\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6284a17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from db_utils.db_utils import print_markdown_table\n",
    "\n",
    "# 6) 결과 확인\n",
    "exec_match_test_df[[\"db_id\",\"gold_result\",\"pred_result\"]].head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a906a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(3):\n",
    "    print(f\"\\n====== ✅ Row {idx} / DB: {exec_match_test_df.iloc[idx]['db_id']} ======\\n\")\n",
    "    \n",
    "    print_markdown_table(\n",
    "        md_str=exec_match_test_df.iloc[idx][\"gold_result\"],\n",
    "        title=\"GOLD RESULT\"\n",
    "    )\n",
    "    \n",
    "    print_markdown_table(\n",
    "        md_str=exec_match_test_df.iloc[idx][\"pred_result\"],\n",
    "        title=\"PREDICTED RESULT\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c9a6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluators.llm_as_judge_exec_match_evaluator import LLMasJudgeExecMatch\n",
    "\n",
    "\n",
    "llm_exec_match_evaluator = LLMasJudgeExecMatch(model_config=model_config_o4_mini)\n",
    "llm_exec_match_pipeline_eval_result = []\n",
    "\n",
    "for i, row in tqdm(exec_match_test_df.iterrows(), ncols=100, colour=\"cyan\", total=len(eval_pipeline_test_df)):\n",
    "    question = row[\"question\"]\n",
    "    gold_result = row[\"gold_result\"]\n",
    "    pred_result = row[\"pred_result\"]\n",
    "\n",
    "    try:\n",
    "        result = llm_exec_match_evaluator(question=question, gold_result=gold_result, pred_result=pred_result)\n",
    "        result = json.loads(result)\n",
    "        llm_exec_match_pipeline_eval_result.append({\n",
    "            \"question\": question,\n",
    "            \"gold_result\": gold_result,\n",
    "            \"pred_result\": pred_result,\n",
    "            \"llm_judgement\": result[\"label\"],\n",
    "            \"reason\": result[\"reason\"]\n",
    "        })\n",
    "        print(f\"##### {i}th DEBUG LOG #####\")\n",
    "        print(\"Question:\", question)\n",
    "        print(\"Gold Result:\", print_markdown_table(gold_result))\n",
    "        print(\"Pred Result:\", print_markdown_table(pred_result))\n",
    "        print(\"LLM Judgement:\", result[\"label\"])\n",
    "        print(\"Reason:\", result[\"reason\"])\n",
    "\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"❌ JSONDecodeError at row {i}: {e}\")\n",
    "        print(\"⚠️ Raw response was:\")\n",
    "        print(repr(result))  # repr을 사용하면 문제되는 이스케이프 문자 확인 가능\n",
    "        llm_raw_sql_pipeline_eval_result.append({\n",
    "            \"question\": question,\n",
    "            \"gold_result\": gold_result,\n",
    "            \"pred_result\": pred_result,\n",
    "            \"llm_judgement\": \"ERROR\",\n",
    "            \"reason\": f\"JSONDecodeError: {str(e)}\"\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Unexpected error at row {i}: {e}\")\n",
    "        llm_raw_sql_pipeline_eval_result.append({\n",
    "            \"question\": question,\n",
    "            \"gold_result\": gold_result,\n",
    "            \"pred_result\": pred_result,\n",
    "            \"llm_judgement\": \"ERROR\",\n",
    "            \"reason\": f\"Exception: {str(e)}\"\n",
    "        })\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6d6e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_exec_match_pipeline_eval_df = pd.DataFrame(llm_exec_match_pipeline_eval_result)\n",
    "fp = len(llm_exec_match_pipeline_eval_df[llm_exec_match_pipeline_eval_df[\"llm_judgement\"] == \"correct\"])\n",
    "tn = len(llm_exec_match_pipeline_eval_df[llm_exec_match_pipeline_eval_df[\"llm_judgement\"] == \"incorrect\"])\n",
    "\n",
    "print(\"Total:\", len(llm_exec_match_pipeline_eval_df))\n",
    "print(\"FP:\", fp)\n",
    "print(\"TN:\", tn)\n",
    "print(fp/tn*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbe1534",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join with Original Eval test set \n",
    "\n",
    "merged_llm_exec_match_pipeline_eval_df = pd.merge(exec_match_test_df, llm_exec_match_pipeline_eval_df, on=\"pred_result\", how='left')\n",
    "merged_llm_exec_match_pipeline_eval_df.drop([\"question_y\",\"gold_result_y\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b658b9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee1bb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_errortypes_count = {}\n",
    "for _, row in merged_llm_exec_match_pipeline_eval_df.iterrows():\n",
    "    error_types = [ x[\"error_type\"] for x in row[\"error_types\"]]\n",
    "    sub_error_types = [ x[\"sub_error_type\"] for x in row[\"error_types\"]]\n",
    "    key_tuples = zip(error_types,sub_error_types)\n",
    "    for key_tuple in key_tuples:\n",
    "        total_errortypes_count[key_tuple] = total_errortypes_count.get(key_tuple,0) +1\n",
    "\n",
    "print(total_errortypes_count)\n",
    "\n",
    "fp_errortypes_count = {}\n",
    "for _, row in merged_llm_exec_match_pipeline_eval_df[merged_llm_exec_match_pipeline_eval_df[\"llm_judgement\"] == \"correct\"].iterrows():\n",
    "    error_types = [x[\"error_type\"] for x in row[\"error_types\"]]\n",
    "    sub_error_types = [x[\"sub_error_type\"] for x in row[\"error_types\"]]\n",
    "    key_tuples = zip(error_types, sub_error_types)\n",
    "    for key_tuple in key_tuples:\n",
    "        fp_errortypes_count[key_tuple] = fp_errortypes_count.get(key_tuple, 0) + 1\n",
    "\n",
    "\n",
    "\n",
    "print(fp_errortypes_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fe1042",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ✅ 에러 딕셔너리 예시 (이 부분은 이미 가지고 있다고 가정)\n",
    "# total_errortypes_count = {...}\n",
    "# fp_errortypes_count = {...}\n",
    "\n",
    "# ✅ 모든 고유 키 수집 및 누락된 키는 0으로 채우기\n",
    "all_keys = sorted(set(total_errortypes_count.keys()).union(set(fp_errortypes_count.keys())))\n",
    "full_dist1 = {key: total_errortypes_count.get(key, 0) for key in all_keys}\n",
    "full_dist2 = {key: fp_errortypes_count.get(key, 0) for key in all_keys}\n",
    "\n",
    "# ✅ 데이터프레임 생성\n",
    "df1 = pd.DataFrame(list(full_dist1.items()), columns=['Error Type', 'Count'])\n",
    "df1['Distribution'] = 'Distribution 1'\n",
    "\n",
    "df2 = pd.DataFrame(list(full_dist2.items()), columns=['Error Type', 'Count'])\n",
    "df2['Distribution'] = 'Distribution 2'\n",
    "\n",
    "# ✅ Error Category, Error Subtype 컬럼 분리\n",
    "df1[['Error Category', 'Error Subtype']] = pd.DataFrame(df1['Error Type'].tolist(), index=df1.index)\n",
    "df2[['Error Category', 'Error Subtype']] = pd.DataFrame(df2['Error Type'].tolist(), index=df2.index)\n",
    "\n",
    "# ✅ 병합\n",
    "combined_df = pd.concat([df1, df2])\n",
    "\n",
    "# ✅ 정렬 기준 설정\n",
    "error_order = combined_df.groupby('Error Subtype')['Count'].sum().sort_values(ascending=False).index\n",
    "\n",
    "# ✅ 절대 Count 기반 시각화\n",
    "plt.figure(figsize=(16, 10))\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "sns.barplot(\n",
    "    x='Count',\n",
    "    y='Error Subtype',\n",
    "    hue='Distribution',\n",
    "    data=combined_df,\n",
    "    order=error_order,\n",
    "    palette='viridis'\n",
    ")\n",
    "plt.title('Comparison of Error Distributions', fontsize=16)\n",
    "plt.xlabel('Number of Errors', fontsize=12)\n",
    "plt.ylabel('Error Subtype', fontsize=12)\n",
    "plt.legend(title='Distribution', fontsize=10, title_fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ✅ 상대 비율 계산\n",
    "df1['Relative Frequency (%)'] = (df1['Count'] / df1['Count'].sum()) * 100\n",
    "df2['Relative Frequency (%)'] = (df2['Count'] / df2['Count'].sum()) * 100\n",
    "\n",
    "df1[['Error Category', 'Error Subtype']] = pd.DataFrame(df1['Error Type'].tolist(), index=df1.index)\n",
    "df2[['Error Category', 'Error Subtype']] = pd.DataFrame(df2['Error Type'].tolist(), index=df2.index)\n",
    "\n",
    "combined_relative_df = pd.concat([df1, df2])\n",
    "\n",
    "# ✅ 상대 비율 기준 정렬\n",
    "relative_error_order = combined_relative_df[combined_relative_df['Distribution'] == 'Distribution 1'] \\\n",
    "    .sort_values(by='Relative Frequency (%)', ascending=False)['Error Subtype']\n",
    "\n",
    "# ✅ 상대 빈도 기반 시각화\n",
    "plt.figure(figsize=(16, 10))\n",
    "sns.barplot(\n",
    "    x='Relative Frequency (%)',\n",
    "    y='Error Subtype',\n",
    "    hue='Distribution',\n",
    "    data=combined_relative_df,\n",
    "    order=relative_error_order,\n",
    "    palette='magma'\n",
    ")\n",
    "plt.title('Comparison of Error Distributions by Relative Frequency', fontsize=16)\n",
    "plt.xlabel('Relative Frequency (%)', fontsize=12)\n",
    "plt.ylabel('Error Subtype', fontsize=12)\n",
    "plt.legend(title='Distribution', fontsize=10, title_fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf4e5a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nl2sql_ssfn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
