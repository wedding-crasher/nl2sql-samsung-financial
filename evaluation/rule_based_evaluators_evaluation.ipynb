{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2de403aa",
   "metadata": {},
   "source": [
    "## 🧪 Exercise 2: Evaluating NL2SQL Model with Rule-Based Metrics\n",
    "\n",
    "In this section, we will evaluate the performance of our NL2SQL model using **rule-based metrics** provided by the **NL2SQL360** library (developed by Dial Lab at HKUST).  \n",
    "We will focus on the following two metrics:\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Metric 1: Execution Accuracy (EX)\n",
    "\n",
    "| Metric               | Evaluation Target                                           | Requires DB Execution? | Notes                                                                 |\n",
    "|----------------------|-------------------------------------------------------------|-------------------------|-----------------------------------------------------------------------|\n",
    "| **Execution Accuracy (EX)** | Compares the execution result of predicted SQL with the ground-truth | ✅ Yes                  | Strict semantic measure. Different SQLs are allowed if results match. |\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Metric 2: Valid Efficiency Score (VES)\n",
    "\n",
    "| Metric               | Evaluation Target                                           | Requires DB Execution? | Notes                                                                 |\n",
    "|----------------------|-------------------------------------------------------------|-------------------------|-----------------------------------------------------------------------|\n",
    "| **Valid Efficiency Score (VES)** | Measures execution efficiency relative to ground-truth SQL | ✅ Yes                  | Combines execution correctness with runtime/memory efficiency.        |\n",
    "\n",
    "- `VES = EX × EfficiencyRatio`\n",
    "- VES penalizes correct SQLs that are inefficient compared to the reference.\n",
    "\n",
    "---\n",
    "\n",
    "These metrics help us go beyond syntax and evaluate whether the predicted SQL is **semantically accurate and computationally efficient**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "158a8345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/azureuser/workspace/nl2sql-samsung-finanical/evaluation/../nl2sql360/src/nl2sql360/__init__.py\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# WARNING: DO NOT INSTALL nl2sql360 VIA pip\n",
    "# ---------------------------------------------------------------\n",
    "# This project uses a **locally customized version** of NL2SQL360\n",
    "# located at: ../nl2sql360/src\n",
    "#\n",
    "# Why?\n",
    "# - Fixed minor bugs in the original implementation\n",
    "# - Added internal debugging and logging features\n",
    "# - Integrated with our custom evaluation pipeline and prompt structure\n",
    "#\n",
    "# ✔️ No need to install the library via pip\n",
    "# ✔️ This will automatically load the local version\n",
    "# ================================================================\n",
    "\n",
    "import sys\n",
    "import importlib\n",
    "\n",
    "# Add local path with customized NL2SQL360 implementation\n",
    "sys.path.insert(0, '../nl2sql360/src')\n",
    "\n",
    "# Import and reload to reflect any recent changes\n",
    "import nl2sql360\n",
    "importlib.reload(nl2sql360)\n",
    "\n",
    "# Confirm that the local path is used (not global site-packages)\n",
    "print(nl2sql360.__file__)  # Should point to ../nl2sql360/src/nl2sql360/__init__.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe4d0ab",
   "metadata": {},
   "source": [
    "## Step 1: Import Dataset\n",
    "\n",
    "Before evaluating the NL2SQL model, you must import a properly structured dataset into the NL2SQL360 pipeline.  \n",
    "We use Core.import_dataset() with DatasetArguments to register the dataset.\n",
    "\n",
    "### 🗂 Expected Directory Structure\n",
    "```\n",
    "../dataset/bird_benchmark/\n",
    "├── dev.json ← Sample file\n",
    "├── dev_tables.json ← (Optional) Table schema info\n",
    "├── dev_databases/ ← Folder containing SQLite DBs\n",
    "│ ├── academic/\n",
    "│ │ └── academic.sqlite\n",
    "│ ├── college/\n",
    "│ │ └── college.sqlite\n",
    "```\n",
    "> Official Bird Dataset: https://bird-bench.github.io/\n",
    "### 📄 Required Files\n",
    "\n",
    "| File | Description |\n",
    "|------|-------------|\n",
    "| dev.json | JSON list of samples. Each entry should contain: <br>• \"question\": natural language query <br>• \"SQL\": gold SQL query <br>• \"db_id\": matching database ID <br>• (Optional) \"difficulty\": difficulty tag for analysis |\n",
    "| dev_databases/ | Directory with subfolders (e.g. academic/, college/) each containing a .sqlite file |\n",
    "| dev_tables.json *(optional)* | JSON file containing database schema (Spider format). Needed for exact-match evaluation |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a8b3737",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-30 05:08:49.594\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mnl2sql360.core.core\u001b[0m:\u001b[36mimport_dataset\u001b[0m:\u001b[36m43\u001b[0m - \u001b[33m\u001b[1mDataset `bird_benchmark` has been already imported.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nl2sql360.core import Core\n",
    "from nl2sql360.arguments import CoreArguments, DatasetArguments\n",
    "\n",
    "\n",
    "core_args = CoreArguments()\n",
    "core = Core(core_args)\n",
    "\n",
    "dataset_args = DatasetArguments(\n",
    "    dataset_name = \"bird_benchmark\",\n",
    "    dataset_dir= \"../dataset/bird_benchmark\",\n",
    "    samples_file= \"dev.json\",\n",
    "    database_dir=\"dev_databases\",\n",
    "    tables_file=\"dev_tables.json\",\n",
    "    question_key=\"question\",\n",
    "    sql_key=\"SQL\",\n",
    "    db_id_key=\"db_id\",\n",
    "    sql_complexity_key = \"difficulty\",\n",
    "    database_domain_file=None\n",
    ")\n",
    "core.import_dataset(dataset_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5e194c",
   "metadata": {},
   "source": [
    "## Step 2: Run Evaluation with Rule-Based Metrics\n",
    "\n",
    "Once the dataset has been registered with `Core.import_dataset()`, you can evaluate model predictions using built-in rule-based metrics such as:\n",
    "\n",
    "- **Execution Accuracy (EX)** — whether the execution results match\n",
    "- **Valid Efficiency Score (VES)** — combines correctness with efficiency (runtime/memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e56e2824",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-30 05:08:51.551\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mnl2sql360.core.core\u001b[0m:\u001b[36mevaluate\u001b[0m:\u001b[36m91\u001b[0m - \u001b[33m\u001b[1mEvaluation `Test_Evaluation_1` on dataset `bird_benchmark` has been existed.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from nl2sql360.core import Core\n",
    "from nl2sql360.arguments import CoreArguments, EvaluationArguments\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "evaluation_args = EvaluationArguments(\n",
    "    eval_name=\"Test_Evaluation_1\",\n",
    "    eval_dataset = \"bird_benchmark\",\n",
    "    eval_metrics = [\"ex\", \"ves\"],\n",
    "    pred_sqls_file = \"../dataset/bird_benchmark/dev.sql\",\n",
    "    enable_spider_eval=True\n",
    "\n",
    ")\n",
    "core.evaluate(evaluation_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932e4369",
   "metadata": {},
   "source": [
    "## Step3: Make a Evaluation Report\n",
    "- There are various way to make a report.\n",
    "- Below I present 2 simple ways to see the result but, you can implement Filter and Scenarios to filter out result for specific query cases. \n",
    "- For various reference and use case, please refer to official library website \n",
    "> Official Library: https://github.com/HKUSTDial/NL2SQL360/blob/master/examples/py_examples/report.py#L5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "180f60b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Evaluation     EX\n",
      "0  Test_Evaluation_1  100.0\n",
      "          Evaluation   VES\n",
      "0  Test_Evaluation_1  2.27\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nSUBQUERY_FILTER = Filter(\\n        name=\"subquery\",\\n        field=Field.SUBQUERY,\\n        operator=Operator.GT,\\n        value=0\\n    )\\n\\nBI_SCENARIO = Scenario(\\n        name=\"BI\",\\n        filters=[Filter(\\'agg\\', Field.AGGREGATION, Operator.GT, 0), Filter(\\'join\\', Field.JOIN, Operator.GT, 0)]\\n    )\\n\\nprint(core.query_overall_leaderboard(dataset_name=\"bird_benchmark\", metric=\"ex\"))\\n\\nprint(core.query_filter_performance(dataset_name=\"bird_benchmark\", metric=\"ex\", filter=filter, eval_name=\"Test_Eval_1\"))\\n\\nprint(core.query_filter_leaderboard(dataset_name=\"bird_benchmark\", metric=\"ex\", filter=filter))\\n\\nprint(core.query_scenario_leaderboard(dataset_name=\"bird_benchmark\", metric=\"ex\", scenario=BI_SCENARIO))\\n\\nprint(core.query_dataset_domain_distribution(dataset_name=\"bird_benchmark\"))\\n\\nprint(core.generate_evaluation_report(dataset_name=\"bird_benchmark\",\\n                                       filters=[SUBQUERY_FILTER],\\n                                       scenarios=[BI_SCENARIO],\\n                                       metrics=[\"ex\", \"ves\"]))\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nl2sql360.core import Core\n",
    "from nl2sql360.arguments import CoreArguments, EvaluationArguments\n",
    "from nl2sql360.filter import Filter, Scenario, Field, Operator\n",
    "\n",
    "    \n",
    "\n",
    "print(core.query_overall_performance(dataset_name=\"bird_benchmark\", metric=\"ex\", eval_name=\"Test_Evaluation_1\"))\n",
    "\n",
    "print(core.query_overall_performance(dataset_name=\"bird_benchmark\", metric=\"ves\", eval_name=\"Test_Evaluation_1\"))\n",
    "\n",
    "\n",
    "\n",
    "# Also there are lots of variations and options for viewing result. Belows are pseudo exmaples for report options.\n",
    "\n",
    "'''\n",
    "SUBQUERY_FILTER = Filter(\n",
    "        name=\"subquery\",\n",
    "        field=Field.SUBQUERY,\n",
    "        operator=Operator.GT,\n",
    "        value=0\n",
    "    )\n",
    "    \n",
    "BI_SCENARIO = Scenario(\n",
    "        name=\"BI\",\n",
    "        filters=[Filter('agg', Field.AGGREGATION, Operator.GT, 0), Filter('join', Field.JOIN, Operator.GT, 0)]\n",
    "    )\n",
    "\n",
    "print(core.query_overall_leaderboard(dataset_name=\"bird_benchmark\", metric=\"ex\"))\n",
    "    \n",
    "print(core.query_filter_performance(dataset_name=\"bird_benchmark\", metric=\"ex\", filter=filter, eval_name=\"Test_Eval_1\"))\n",
    "    \n",
    "print(core.query_filter_leaderboard(dataset_name=\"bird_benchmark\", metric=\"ex\", filter=filter))\n",
    "\n",
    "print(core.query_scenario_leaderboard(dataset_name=\"bird_benchmark\", metric=\"ex\", scenario=BI_SCENARIO))\n",
    "    \n",
    "print(core.query_dataset_domain_distribution(dataset_name=\"bird_benchmark\"))\n",
    "\n",
    "print(core.generate_evaluation_report(dataset_name=\"bird_benchmark\",\n",
    "                                       filters=[SUBQUERY_FILTER],\n",
    "                                       scenarios=[BI_SCENARIO],\n",
    "                                       metrics=[\"ex\", \"ves\"]))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8243ec9e",
   "metadata": {},
   "source": [
    "## Step4: Clear History Cache\n",
    "- There are various way to make a report.\n",
    "- Below I present 2 simple ways to see the result but, you can implement Filter and Scenarios to filter out result for specific query cases. \n",
    "- For various reference and use case, please refer to official library website \n",
    "> Official Library Example: https://github.com/HKUSTDial/NL2SQL360/blob/master/examples/py_examples/report.py#L5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e9223ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-30 05:09:40.728\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mnl2sql360.core.core\u001b[0m:\u001b[36mdelete_evaluation_history\u001b[0m:\u001b[36m519\u001b[0m - \u001b[33m\u001b[1mYou are deleting the evaluation history. Please enter `Y` / `YES` to confirm or enter `N` / `NO` to cancel the operation. \u001b[0m\n",
      "\u001b[32m2025-07-30 05:09:43.919\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mnl2sql360.core.core\u001b[0m:\u001b[36mdelete_evaluation_history\u001b[0m:\u001b[36m537\u001b[0m - \u001b[32m\u001b[1mDelete evaluation `Test_Evaluation_1` for dataset `bird_benchmark` successfully.\u001b[0m\n",
      "\u001b[32m2025-07-30 05:09:43.920\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mnl2sql360.core.core\u001b[0m:\u001b[36mdelete_dataset_history\u001b[0m:\u001b[36m492\u001b[0m - \u001b[33m\u001b[1mYou are deleting the dataset history. Please enter `Y` / `YES` to confirm or enter `N` / `NO` to cancel the operation. \u001b[0m\n",
      "\u001b[32m2025-07-30 05:09:45.955\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mnl2sql360.core.core\u001b[0m:\u001b[36mdelete_dataset_history\u001b[0m:\u001b[36m515\u001b[0m - \u001b[32m\u001b[1mDelete dataset `bird_benchmark` successfully.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "core.delete_evaluation_history(\n",
    "    dataset_name=\"bird_benchmark\",\n",
    "    eval_name=\"Test_Evaluation_1\"\n",
    "    )\n",
    "core.delete_dataset_history(\n",
    "    dataset_name=\"bird_benchmark\",\n",
    "    delete_relavant_evaluations=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9eb17d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nl2sql_ssfn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
